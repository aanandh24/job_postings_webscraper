{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac4753d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from html.parser import HTMLParser\n",
    "from io import StringIO\n",
    "import ssl\n",
    "import urllib3\n",
    "import warnings\n",
    "\n",
    "# Suppress only the InsecureRequestWarning from urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Disable SSL verification globally for urllib if you need to\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs = True\n",
    "        self.text = StringIO()\n",
    "\n",
    "    def handle_data(self, d):\n",
    "        self.text.write(d)\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.text.getvalue()\n",
    "\n",
    "\n",
    "class LeverWebScraper:\n",
    "    def __init__(self):\n",
    "        self.urls = None\n",
    "\n",
    "    def extract_urls(self, text):\n",
    "        regex_pattern = r\"(https?://[jobs.lever.co][^\\s]+)\"\n",
    "        urls = re.findall(regex_pattern, text)\n",
    "        return urls\n",
    "\n",
    "    def remove_tags(self, html):\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        for data in soup(['style', 'script']):\n",
    "            data.decompose()\n",
    "        return ' '.join(soup.stripped_strings)\n",
    "\n",
    "    def get_lever_sites(self):\n",
    "        lever_gsheet = \"https://docs.google.com/spreadsheets/d/18u2sKRKjKz9gwRyob0p9KmcyVC6NX8JaJhjOqsRmbKY/edit?usp=sharing\"\n",
    "        # Add `verify=False` to bypass SSL verification\n",
    "        res = requests.get(url=lever_gsheet, verify=False)\n",
    "        content = res.content\n",
    "        content = self.remove_tags(content)\n",
    "        urls = self.extract_urls(content)\n",
    "        return urls\n",
    "\n",
    "    def strip_tags(self, html):\n",
    "        s = MLStripper()\n",
    "        s.feed(html)\n",
    "        return s.get_data()\n",
    "\n",
    "    def clean_job_posting(self, posting):\n",
    "        text = self.strip_tags(posting)\n",
    "        cleaned_text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n",
    "        return cleaned_text\n",
    "\n",
    "    def get_postings(self):\n",
    "        all_postings = list()\n",
    "        map0 = {\n",
    "            'urls': [],\n",
    "            'descriptions': [],\n",
    "            'points': []\n",
    "        }\n",
    "        links = self.get_lever_sites()\n",
    "        links = [l + \"/\" if not l.endswith(\"/\") else l for l in links]\n",
    "        links = [l for l in links if \".eu.\" not in l]  # Exclude European sites\n",
    "        links = list(set(links))\n",
    "        random.shuffle(links)\n",
    "\n",
    "        for current_url in links:\n",
    "            try:\n",
    "                # Add `verify=False` to bypass SSL verification\n",
    "                res = requests.get(current_url, headers={'User-Agent': 'Mozilla/5.0'}, verify=False)\n",
    "                html_page = res.content\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Request error for {current_url}: {e}\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(html_page, 'html.parser')\n",
    "            postings = soup.select(\"div.posting-title h5.posting-name\")\n",
    "\n",
    "            for posting in postings:\n",
    "                clean_posting = self.clean_job_posting(str(posting))\n",
    "                clean_posting = clean_posting.replace('Sr . ', 'Sr. ')\n",
    "                points = 0\n",
    "\n",
    "                if \"Data Analytics\" in clean_posting:\n",
    "                    points += 3\n",
    "                if \"REMOTE\" in clean_posting:\n",
    "                    points += 3\n",
    "                if \"United States\" in clean_posting:\n",
    "                    points += 3\n",
    "\n",
    "                skip_posting = False\n",
    "                keywords_to_exclude = [\"on - site\", \"onsite\", \"hybrid\"]\n",
    "\n",
    "                for w in keywords_to_exclude:\n",
    "                    if w in clean_posting.lower():\n",
    "                        skip_posting = True\n",
    "                        break\n",
    "\n",
    "                if skip_posting:\n",
    "                    continue\n",
    "\n",
    "                map0['urls'].append(current_url)\n",
    "                map0['descriptions'].append(clean_posting)\n",
    "                map0['points'].append(points)\n",
    "                print(current_url, '\\t', clean_posting)\n",
    "\n",
    "            time.sleep(1.0)\n",
    "        return map0\n",
    "\n",
    "\n",
    "# Usage\n",
    "lever_scraper = LeverWebScraper()\n",
    "postings_map = lever_scraper.get_postings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab506da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urls</th>\n",
       "      <th>descriptions</th>\n",
       "      <th>points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [urls, descriptions, points]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "# Create DataFrame from the postings map\n",
    "df = pd.DataFrame(postings_map)\n",
    "\n",
    "# Sort the DataFrame by points in descending order\n",
    "df = df.sort_values(by=['points'], ascending=False)\n",
    "\n",
    "# Filter the DataFrame to show only rows where points > 5\n",
    "df_filtered = df[df['points'] > 5]\n",
    "\n",
    "# Set display options and show filtered results\n",
    "from pandas import option_context\n",
    "\n",
    "with option_context('display.max_colwidth', 400):\n",
    "    display(df_filtered.head(50))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
